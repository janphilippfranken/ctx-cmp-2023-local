hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  # sweep:
  #   dir: .
  #   subdir: .
  # job_logging:
  #   root:
  #     level: INFO
  job:
    env_set:
      TOKENIZERS_PARALLELISM: "false" 
      CUDA_VISIBLE_DEVICES: "0,1"  # make sure to keep this number small to avoid crash on ccn!
      WANDB_DISABLED: true

defaults:
  - base_config
  - _self_

model:
  model_name_or_path: distilgpt2 # bigscience/bloomz-560m # change to smaller model for debugging
  cache_dir: .cache/ # only uncomment this if not working on ccn otherwise crash in home directory!
  resize: True # if we want to resize the model block size
  block_size: 8 # new block size

training:
  generation_max_length: 28
  log_level: critical
  do_train: True
  overwrite_output_dir: False
  max_steps: 10 

data:
  dataset_name: stas/openwebtext-10k  # the dataset 
  cache_dir: .cache/  # where we cache the dataset (be careful not to cache in home directory on ccn!)
  resize: True  # if we want to resize the dataset
  size: 100   # size of the random subset from the resized dataset